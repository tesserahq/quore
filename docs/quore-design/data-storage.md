## Data Storage and Indexing with PGVector and LlamaIndex

Efficient storage and retrieval of vector embeddings is central to the platform’s ability to provide relevant context to the LLM. We use **PostgreSQL with PGVector** as the vector database, which allows us to keep all tenant data (both relational and vector) in one scalable DB engine. This approach simplifies architecture by treating vector embeddings as just another type of data in Postgres . Below are key considerations and design decisions for data storage:

- **Postgres and PGVector Setup:** The PostgreSQL database is extended with the PGVector plugin, enabling a new column type (usually VECTOR) for embedding storage and indexing methods for similarity search. We will create one or more tables to store document embeddings. As mentioned, a straightforward schema is an embeddings table with columns: id (PK), workspace_id (FK), project_id (FK), embedding VECTOR(d) (where d is the dimensionality of the embedding, e.g. 1536 for OpenAI embeddings), and perhaps document_id or text. We will create a **PGVector index** (ivfflat or similar) on the embedding column, potentially partitioned by workspace or project for performance. With this, we can perform queries like: SELECT \* FROM embeddings WHERE project_id = X ORDER BY embedding &lt;-&gt; query_vector LIMIT 5; to get the top-5 closest vectors to a given query vector. This is all inside Postgres, so we benefit from its reliability and consistency. Moreover, combining filters (e.g. only embeddings from a certain document type) is possible with SQL conditions.

- **Document Storage:** The actual content of documents can be stored either within Postgres or externally. A simple method is to store chunk texts or references in the embeddings table or a related table. For instance, we might have a separate documents table for each project or a unified one keyed by project, where each entry has document_id, project_id, original text or a link to the file, etc. During ingestion, each document is split into chunks and each chunk’s embedding is stored. We can store the chunk text alongside the embedding for quick retrieval, or store just an ID and fetch the text from the documents table after finding the nearest vectors. Both approaches are viable; storing text in the vector table trades some duplication for query simplicity. Given modern hardware and the likely moderate size of text chunks, this is acceptable.

- **LlamaIndex usage:** **LlamaIndex** will be used to facilitate both the indexing (ingestion) process and the query (retrieval + synthesis) process. In the ingestion phase, LlamaIndex provides connectors to various data sources (files, Notion, web pages, databases, etc.), and utility to break them into chunks, clean the text, and embed them using a chosen embedding model. The design is flexible on which embedding model is used – it could be OpenAI’s text-embedding-ada model, or HuggingFace sentence transformers, etc., configured per project. LlamaIndex will generate embeddings for each chunk and call a **PGVector storage layer** to persist them. (LlamaIndex supports custom vector stores; we will implement or use an existing PGVector VectorStore class so that LlamaIndex’s index.save_to_disk() or similar calls actually write to Postgres.)

When a new document is added to a project (via an API endpoint or a pipeline trigger), LlamaIndex is invoked: it loads the doc, splits into nodes, and upserts those into PGVector. We maintain metadata so that if a document is deleted or updated, we can find and remove/update its embeddings as well.

- **Retrieval workflow:** When an LLM query comes in, before or as part of the LangChain agent, we often want to retrieve relevant context from the project’s indexed knowledge. We use **LlamaIndex query engines** or LangChain’s retrieval QA to do this. Essentially, we embed the user’s question with the same embedding model (which LlamaIndex can do on the fly) to get a query vector. Then we query PGVector for the nearest neighbor embeddings in that project (this can be done via an SQL query or via LlamaIndex’s VectorStoreIndex query interface if using their abstraction). PGVector returns, say, the top 5-10 most similar chunks. We then fetch the associated text for those chunks (either directly from the vector store if stored, or by joining on a documents table). These chunks of text are then provided to the LLM as context.

For example, the system might construct a prompt like: “Here are relevant excerpts from our knowledge base:\\n\[…texts…\]\\nGiven this information, answer the question: …”. If using LangChain, this could be done by a RetrievalQA chain. If using LlamaIndex’s query engine, it might directly do a synthesis (they have response builders that can take the top nodes and generate an answer).

- **Pipeline Customization:** The design leaves room for different retrieval strategies on a per-project basis. A project’s config might specify a hybrid search (combine keyword search and vector search), or use filters (e.g., only look at recent documents). Because we use Postgres, we can leverage SQL for filtering and even hybrid queries (for instance, PGVector allows combining vector similarity with arbitrary WHERE clauses). LlamaIndex can be extended to apply such filters as well. For now, the primary method is vector similarity, but nothing prevents adding metadata fields (like tags, timestamps) to the embeddings and filtering by them.

- **Index Maintenance:** Over time, projects may accumulate many embeddings. PGVector indices should be periodically maintained (e.g., reindex or adjust the index parameters for efficiency). The design could include background jobs for cleaning up stale embeddings (if documents removed) or for rebuilding indexes if the embedding model changes. These are operational details; the design ensures we track embeddings per project so such maintenance can be scoped correctly.

- **Why PGVector (design rationale):** Using PGVector in Postgres allows us to avoid introducing a separate vector DB service. This _“minimizes both the complexity of the application architecture and the number of requests made to different data stores”_, as one developer noted about using pgvector . We get transactional updates (document and its embeddings can be inserted transactionally in one commit) and consistent backups along with the rest of the data. It also eases multi-tenant data modeling: we can use Postgres schemas or simple foreign keys to segregate tenant data rather than managing API keys across different vector DB instances. The trade-off is that extremely high-scale vector search might be slightly slower than a specialized vector DB, but PGVector has proven efficient for many use cases and our data volumes per tenant are expected to be manageable (and can be optimized with index tuning).

- **Non-Vector Data:** Besides embeddings, the database also stores the metadata for Workspaces, Projects, Clients, and Plugins as regular relational tables. These have relatively low volume (compared to embeddings) and are straightforward to model (e.g., a workspaces table, a projects table with a foreign key to workspaces, etc.). Using Postgres for all of these means we maintain strong consistency on relationships (e.g., if a workspace is deleted, cascade delete its projects, etc.). Access control at the application layer further ensures that queries always include the appropriate workspace filter, but we could also use Postgres row level security for an extra safety layer if desired.

### Vector Store Organization Strategies (Project-Level Indexing)

In a multi-tenant LLM platform using PGVector, deciding how to organize the vector store (embeddings index) per project is crucial. The two main approaches are: using a single shared embeddings table for all projects (distinguished by a workspace_id/project_id), or creating dedicated tables for each project (or for each embedding model). Each strategy involves trade-offs in terms of schema design, flexibility, and maintenance.

- **Single Shared Embeddings Table:** All embeddings reside in one table, with columns to identify the workspace/project. This **simplifies schema management** (only one table and index to maintain) and can work well if all projects use the **same embedding model** (same vector dimension). However, it becomes problematic if different projects use different embedding models. PGVector requires a fixed vector length per column – e.g. a column defined as vector(1536) only accepts 1536-dimension vectors . If one project uses 768-d vectors and another uses 1536-d, they cannot share the exact same column without hacks. A unified table would either need to **fix one dimension for all** (forcing all embeddings to pad or truncate to that length) or forgo indexing by using a variable-length vector type (PGVector now allows vector without a specified dimension) . Forgoing a fixed dimension means you **lose the ability to index** that column across all entries , defeating the purpose of fast similarity search. Another challenge is query performance and isolation: filtering by project or tenant in a large shared index is non-trivial. PGVector’s similarity search indexes (like HNSW or IVF) don’t inherently support per-row filtering, so cross-tenant queries might return irrelevant results unless every query includes a filter on project_id. In practice, adding such filters can prevent using the index efficiently . This means a shared table may require workarounds (like separate partial indexes per project or relying on Postgres row-level security filters). Maintenance can also become complex as the table grows very large with many projects’ data.

- **Per-Project Tables:** Each project has its own embeddings table (and index), isolated from others. This **ensures data isolation and security** by design (one project cannot accidentally query another’s vectors) and allows each table to be tailored to the project’s chosen embedding model. Different projects can use different embedding dimensions without conflict, since each table’s vector column is defined separately (e.g. one table with vector(768), another with vector(1536)). This approach aligns well with PGVector’s limitations – you simply avoid mixing different dimensions in one schema. It also simplifies indexing and querying: no need for runtime filtering by project, since each query naturally targets the project’s own table. The downside is **operational overhead** if there are many projects. Each new project means creating a new table and index. For a platform with hundreds of projects, that is hundreds of tables (though Postgres can handle many tables, it adds complexity for migrations and upkeep). Still, this pattern is commonly recommended for multi-tenant vector search; it sidesteps the filtering issue and keeps indexes smaller and focused . Storage overhead for many small tables is usually negligible compared to one huge table, and queries per project remain fast.

- **Per-Model Tables:** A variation is to partition by embedding model rather than by project. All projects using Model A share one embeddings table, while Model B has a separate table, etc. This reduces the total number of tables if multiple projects use the same model/dimension. It also centralizes indexing efforts per model. However, you’d still need to distinguish projects within those tables (e.g. a project_id column as part of the key), meaning intra-table filtering by project is still required (though only among projects using that model). Essentially, this trades some isolation for fewer tables. It handles **different vector dimensions** by design (each model’s table has the appropriate dimension), but projects using the same model are co-mingled. Unless the platform has a small, fixed set of models, this approach can become a partial measure – in the worst case, if every project chooses a unique model, you end up with per-project tables anyway. In practice, per-model grouping might make sense if the platform only supports a limited catalog of embedding models (e.g. OpenAI Ada 1536-d for most, and maybe a 768-d model for others). It provides some efficiency in not duplicating tables for the same schema, but at the cost of reintroducing multi-tenant sharing concerns within those groups.

### Embedding Model Flexibility vs. Uniformity

An important design decision is whether to **standardize on a single embedding model** for all projects or allow each project to choose its own. Using a single embedding model globally simplifies the system greatly: all embeddings have the same dimension and live in the same semantic space. This means one consistent schema and index, and the query pipeline is uniform (every query embedding is generated by the same model as the stored embeddings). The trade-off is **loss of flexibility**. Different use cases might benefit from different models – for example, a smaller, domain-specific embedding model might outperform a generic larger one for certain data . If every project is forced to use the same model, some might get suboptimal results. On the other hand, allowing flexible models per project introduces complexity: the platform must handle multiple embedding generators and **multiple vector spaces**. Practically, that means keeping track of which model each project (or each index) uses, so that queries use the correct embedding model to encode the prompt/document. It also means dealing with different vector dimensions, as noted above. One cannot mix arbitrary vectors in one index; _different embedding models with different dimensional outputs need isolated storage and indexes_ . Enforcing a single embedding model avoids this problem entirely (one dimension everywhere), but at the cost of adaptability. A middle ground could be to support a few approved models – e.g. the platform might default everyone to Model X but allow advanced users to opt into Model Y – in which case the system must implement the multi-model table strategy or a similar namespacing mechanism to keep embeddings separate.

From a **maintenance perspective**, a single global model is easier initially – there’s no need to store a model identifier with each vector or to dynamically switch embedding functions. All projects benefit from improvements to that one model. However, if that model becomes outdated or if a clearly superior model emerges, the platform would need to **upgrade all projects in lockstep** (see next section). Conversely, with per-project model flexibility, the platform could roll out new models incrementally (some projects upgrade while others stay on the old model). This flexibility can be a selling point but requires careful design to manage multiple versions concurrently.

### Model Upgrades and Re-indexing Considerations

Over time, embedding models will evolve. New versions or completely new models may offer better accuracy or smaller vector sizes. The design must workspace for how to upgrade embeddings **without disrupting the platform**. If using a single embedding model for all data, an upgrade essentially means a **full re-indexing**: every stored document’s embedding needs to be recomputed with the new model, because queries must use the same embedding space as the index. If the new model has a different vector dimension, this is a major schema migration (for example, going from a 1536-d model to a 2048-d model would require altering the column type or creating a new table). Even if the dimension is the same, the numeric values change, so all vectors must be replaced. During such an upgrade, search results might degrade unless handled carefully (since old and new embeddings are not directly comparable). One approach is to run two sets of embeddings in parallel: keep the old model’s index while building a new index with the upgraded model, and then switch over when ready – this is essentially **embedding versioning**. For now, our platform will adopt a simpler strategy (one active embedding per project) and accept that an upgrade means re-embedding the data offline. We will ensure the **raw document data is stored separately** so that we can regenerate embeddings as needed. All original source texts (or other data) will be retained in a natural form (e.g. stored in a separate table or storage service, not just as vectors). This follows best practices: always keep the raw data so you can reprocess it with improved models later . That way, if we decide to upgrade the embedding model for a project (or globally), we can iterate over all documents, compute new embeddings, and rebuild the index. The platform should schedule this re-indexing in a maintenance window or in the background, to minimize user impact.

Although we do not plan to support multiple embedding versions simultaneously at launch, we acknowledge that **embedding versioning** could be valuable in the future. Versioning would mean tagging or segregating embeddings by the model/version that generated them. This allows for A/B testing new models on a subset of queries or a subset of projects, and provides a safety net to rollback to the previous embeddings if needed. In industry scenarios, teams often A/B test a new embedding model against the old one – they index the same content with both models and compare retrieval performance . This requires either a separate index (per model) or a version field if sharing a store. Similarly, if an upgrade yields worse-than-expected results, having the old embeddings on hand (and tagged as a prior version) makes it easier to revert. Our design is mindful of this: we isolate embeddings by project/model, which inherently supports side-by-side indexes if necessary. While we won’t implement explicit version identifiers in the schema right now, the architecture (especially with per-project tables) makes it possible to introduce **namespaced indices** or version tags later. For example, we could create a new embeddings table or column for a new model and keep the old one until a switch is confirmed. In summary, **embedding versioning is not required at this stage but remains a future enhancement** to facilitate non-disruptive upgrades and experiments (e.g. blue/green deployments of a new embedding model for testing or easy rollback).

### Storing Raw Documents for Future Re-indexing

As mentioned, a key best practice for long-term maintainability is storing the original (“natural”) document data outside of the vector index itself. The vector store (PGVector) holds the embeddings and maybe some metadata, but it should not be the sole repository of the content. We will maintain a separate data store (or database table) for raw documents, text chunks, or other source data that were used to generate embeddings. This separation is important because if we need to **re-index (re-embed) the data** – due to an embedding model upgrade, a bug fix in the embedding pipeline, or an improvement in chunking strategy – we can do so directly from the source texts. For instance, if a project has uploaded documents, those documents (or their pre-processed segments) will be stored in a documents table keyed by project and document ID. The embeddings table will reference these IDs but not serve as the single source of truth for content. By having the raw data, we avoid the pitfall of “embedding lock-in,” where one might be tempted to use the existing vectors as the only archive of information. Should vectors become outdated or need recalculation, we simply pull the raw data and run the new embedding process. This design also opens the door to building improved indexes in the future (for example, adding new metadata or using a different embedding model) without requiring users to re-upload anything. It aligns with the expectation that embedding models will continuously improve and that re-indexing will be periodically beneficial . In summary, **document storage and vector indexes are decoupled**: the platform stores original content (for auditability and reprocessing) and uses PGVector tables as a derived index that can be rebuilt as needed. This separation of concerns ensures we can evolve the vector index schema or embedding techniques over time while preserving user data and enabling smooth re-indexing workflows.

### Managing Source Data and Metadata in a Multi-Tenant Vector Index

**Project-Scoped Datasets for Original Data**

Each **project** serves as an isolated tenant context for data and vector search. Within a project, we introduce the concept of **datasets** to group and manage source data. A dataset represents a collection of original (“natural”) data items (e.g. documents, knowledge base articles, records) that will be embedded into vectors. For each dataset, the platform stores the _raw content_ that was used to generate embeddings. This could be stored as text blobs in the database or as references to external storage, but it is critical that the original data is retained for future re-indexing and reference. By keeping the raw text/content alongside the vectors, the system can later re-create or update embeddings (for example, if a new embedding model is adopted or the data is edited) without needing the user to re-upload data . Storing original data in the vector store is a common practice – for instance, [LanceDB](https://lancedb.github.io/lancedb/notebooks/reproducibility/) allows storing not only vectors but also the original objects like text or images alongside them – which ensures that search results can be contextualized (by returning the original text snippet) and that embeddings can be refreshed as needed.

In practical terms, a **Dataset** entity (scoped to a project) might have attributes like a unique dataset ID, a name, and a type (e.g. “static” for uploaded documents or “dynamic” for externally fetched data). All data items belong to a dataset. This grouping helps with organization (you can have separate datasets for, say, “Product Manuals” vs. “Support Tickets”) and provides a handle for bulk operations (you might re-index or delete all vectors from a specific dataset in one go). It also serves as a logical filter if needed – for example, you could limit a query to one dataset by using a dataset label or ID filter.

Each data item in a dataset (for example, a document) can be **chunked** into smaller pieces for embedding. The _original text for each chunk_ is stored in the database along with its vector. Storing the exact chunk content ensures that if you need to re-embed that chunk (or provide it as an answer context), you have the text readily available. This design is reflected in our schema (see **Schema Design** below), where each vector row includes a field for the raw text content of the chunk . If the original data is non-text (images, etc.), a reference or ID to the object can be stored instead, so that you can retrieve the object when needed.

### Handling Static vs. Dynamic Data Sources

Not all data is static or user-provided. The platform must also support **dynamic data sources** – information that changes frequently or is fetched on-demand from external services. Examples include customer-specific infrastructure data (like virtual machines, cloud resources, or real-time records that belong to a user). We handle this by allowing special datasets that are updated at runtime.

For **static datasets** (e.g. a collection of documents uploaded to the project), the ingestion is typically a one-time or occasional process. A user adds new documents to a dataset, the system splits them into chunks, generates embeddings, and inserts those into the project’s vector index (with the content and any metadata stored). These vectors remain mostly fixed until the data changes or a re-index is triggered.

For **dynamic datasets**, the platform will not pull or fetch data directly from external APIs or services. Instead, a dedicated ingestion endpoint will be provided to accept frequently changing data. External systems—such as cloud infrastructure APIs—can periodically push updates to this endpoint. For example, a dataset representing “VMs assigned to users” would not be populated by file uploads or live queries, but by external services ingesting current data into the platform on a schedule. The ingestion workflow for dynamic data might look like:

1. **Data Push**: On a defined schedule, an external service sends the latest list of items (e.g., all VMs for the current user or project) to the ingestion endpoint.

2. **Embedding Generation:** For each item, a description or relevant attributes are compiled into a text snippet (the “natural language” representation of that item), which is then converted to an embedding vector via the LLM’s embedding model.

3. **Upsert into Vector Store:** The vector is inserted or updated in the PGVector index for that project, in a dataset designated for that dynamic source. If the item (VM) already exists, its vector entry is updated; if it’s new, a new entry is added; if an item was removed (e.g. a VM terminated), the corresponding vector can be removed or marked inactive.

4. **Store Original Data:** The content used for the embedding (e.g. “VM #123 – type: t2.micro, status: active, region: us-west-2, …”) is stored as the chunk text for reference. This way, even dynamic info is traceable and the exact indexed content is preserved. In cases where storing the full content isn’t feasible (perhaps the data is very large), the system can store an identifier and minimal info, relying on the external source for full details. But typically, a concise textual summary of the item is what gets embedded and stored.

Dynamic data requires the system to handle **frequent updates**. The design should consider mechanisms for updating vector entries in place or invalidating old vectors. Since each vector entry can be keyed by an identifier (for example, a VM’s ID from the external system), the platform can update that entry’s embedding and metadata when the external data changes. This is more efficient than deleting and re-adding many vectors from scratch. In our schema, we might include a unique key for each data item (or use the external ID as part of the primary key) so that upserts are possible.

One challenge with dynamic data is deciding when to (re)index. The platform might choose to **index on the fly** – for instance, just before a query, fetch the latest data for that user and update the index. Alternatively, it might maintain a near-real-time sync (e.g. subscribe to events from the external service and update vectors as changes occur). The strategy can vary by use case, but the data model supports both: the vector store can be incrementally updated any time, and old embeddings can be replaced with new ones as needed.

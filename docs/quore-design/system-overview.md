# Quore

Multi-Tenant AI Web Platform Architecture

### Architecture Overview

_Figure: High-level MCP client–server architecture. The platform acts as the “Host” with an integrated MCP client connecting to external MCP Servers (plugins) that expose Tools (and optionally Resources/Prompts) to the LLM . The MCP transport layer (e.g. stdio or SSE) enables two-way communication._

The platform is built as a **multi-tenant AI web service** using **FastAPI** as the HTTP interface layer. Under the hood, it orchestrates LLM operations with **LangChain** and **LlamaIndex**, and uses **PostgreSQL** (with the **PGVector** extension) for persistent data storage and vector similarity search. The overall architecture follows a modular design:

- **FastAPI** handles all HTTP requests (from external clients and user applications) and routes them to the appropriate tenant workspace logic. It provides authentication, request parsing, and response formatting (JSON, streaming, etc.), but the core LLM and plugin logic lives in the service layer.

- **Service Layer (LLM Orchestration)** comprises components using LangChain and LlamaIndex. This layer implements the AI workflows: retrieving relevant context from data via vector search, invoking LLMs, and utilizing plugins as tools when needed. LangChain’s agent mechanism is used to integrate external Tools (plugins) into the LLM’s reasoning process, while LlamaIndex manages document indexing and retrieval for knowledge augmentation.

- **Data Layer (Postgres + PGVector)** provides a multi-tenant-aware database. All relational data (Workspaces, Projects, Clients, plugin metadata, etc.) and vector embeddings are stored in Postgres schemas/tables, keyed by workspace/project to enforce isolation. PGVector extends Postgres to support efficient embedding similarity queries (cosine/IP distance), treating vector embeddings “like just another column in your Postgres table,” which simplifies the architecture and minimizes external dependencies . This unified store avoids the complexity of maintaining a separate vector database.

- **Plugin Subsystem (MCP Integration)** enables extensibility via external tool APIs. The platform acts as an **MCP Host** that can connect to multiple **MCP servers** (plugins) to give the LLM controlled access to external systems . Each plugin runs as an independent service (MCP server) and the platform maintains an MCP client connection to it. When the LLM needs to use a tool, the MCP client will send the request to the plugin server and return the result to the LLM agent.

**Multi-tenancy:** The architecture is inherently multi-tenant – each **Workspace** (tenant organization) has isolated data and resources. All database tables include a workspace_id (tenant identifier) to segregate data; shared services are built to handle multiple tenants by scoping operations to the appropriate tenant_id . FastAPI’s authentication layer will determine the caller’s workspace (e.g. from an API key or token tied to a Workspace/Client) and ensure all subsequent operations (vector queries, document access, plugin use) are executed in that tenant’s context. This design guarantees that data and plugins configured in one workspace are never accessible from another, providing isolation similar to a “workspace-per-tenant” model.

**Scalability and Modular Components:** Each major function (vector search, LLM invocation, plugin call) is designed as a decoupled component. For example, the vector search uses PGVector (which can scale within Postgres and even be sharded or partitioned by tenant if needed), and the plugin calls are abstracted via MCP clients (which could run in separate processes or threads). In the current design, FastAPI hosts the orchestration logic in a monolithic service for simplicity, but the boundaries (API layer, LLM service logic, database, plugins) could be separated into microservices if scaling demands it. For now, the FastAPI app will integrate the LangChain/LlamaIndex logic directly, and use asynchronous IO to handle calls to the database and plugin network endpoints efficiently.

**Core Entities and Data Model**

The system revolves around four core entity types – **Workspaces**, **Projects**, **Clients**, and **Plugins** – and their relationships. The diagram below outlines the data model (each Workspace contains multiple Projects and Clients, and can utilize multiple Plugins):

- **Workspaces (Tenant Organizations):** A Workspace represents an organization or tenant. It is the top-level container for all data and settings for a given tenant. Each workspace has attributes like a unique id, a name, and configuration settings (e.g. default models or settings applicable across the org). All other entities (projects, clients, plugin usage) are _scoped by workspace_. In the database, every table that relates to tenant-specific data will include a foreign key to workspace_id to enforce isolation . Administrators at the workspace level can manage the projects and plugins for that tenant. Workspaces do not directly contain LLM logic themselves; instead, they organize projects and control access (for example, one workspace cannot access another’s data because all queries filter by the workspace’s ID). This design aligns with multi-tenant SaaS best practices where a single application instance serves multiple customers by partitioning data per tenant.

- **Projects:** A Project is a subdivision within a Workspace that encapsulates a specific **dataset or application context**. Each project is typically associated with a distinct collection of data to index and a distinct LLM configuration or purpose. For example, a workspace might have separate projects for “Internal Knowledge Base Q&A”, “Website FAQ Chatbot”, and “HR Document Analysis” – each project having its own documents and maybe different pipelines. A Project has fields like id, workspace_id (linking to its Workspace), a name, and **configuration JSON** that defines its indexing and retrieval pipeline. The configuration may include:
  - Data source definitions (e.g. file repositories, databases, web sources) that should be ingested.
  - Embedding settings: which embedding model to use, any text pre-processing (splitting, cleaning) steps, etc.
  - LLM prompt or chain settings: e.g. a specific system prompt or chain type if needed for this project’s queries.
  - **Enabled Plugins**: a list of tools (from the Plugin registry) that this project can utilize in queries.

Conceptually, a Project corresponds to a **LlamaIndex index** or a group of indexes. During data ingestion, documents associated with the project are processed (split into chunks, embeddings generated via LlamaIndex or LangChain) and stored in the PGVector index tied to that project. By scoping queries to a project’s vectors, we retrieve only relevant embeddings for that project’s context. See **Vector Store Organization Strategies** for more details  
<br/>In addition to the vector index, Projects could also maintain metadata about the original data (e.g., a documents table listing each source file or record added, with fields for title, source type, etc.). This helps in updating or re-building indexes. But the core retrieval uses PGVector for similarity search. In summary, Projects delineate different indexes/pipelines under a workspace, each with potentially different data and tools.

- **Clients:** Clients represent external entities (applications, services, or users) that interact with the platform via the API. Essentially, a Client is an **authentication principal** that is scoped to a single Workspace. For example, if an organization (Workspace) builds an internal application that queries the AI platform, that app would be registered as a Client with an API key or credentials allowing it to call the platform’s endpoints. Each client has an id, belongs to a workspace_id, and has auth credentials (such as an API key or OAuth token information). We might also store a human-friendly name and perhaps permission scopes (e.g. read-only vs read-write if certain operations are allowed).  
   <br/>When a Client makes API calls to FastAPI (for instance, asking a question to an LLM agent in a specific project), the provided API key or token maps to a Client record and thus to a workspace. This mechanism ensures the request is executed in the correct tenant context. The **Client does not directly reference Projects**, but the API requests from the client will specify which project or use-case they are targeting (e.g., as a parameter or endpoint path). The platform will verify that the project indeed belongs to the client’s workspace. This design allows one workspace to have multiple client integrations (for example, one for a frontend app, one for a backend service, etc.) while all being confined to that workspace’s data and tools.  
   <br/>Clients are primarily about security and integration management – e.g. rotating keys, tracking usage per client, and potentially implementing rate limits or quotas per client. They do not have any LLM logic themselves; they simply authenticate and then invoke the platform’s functionality on behalf of a workspace user or application.

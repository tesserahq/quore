## Schema Design and Indexing in PGVector

The platform uses **PostgreSQL with the PGVector extension** as its vector store, enabling semantic search over project-specific data. This is part of a multi-tenant architecture structured around **workspaces**, **projects**, and LLM-powered tooling.

### **Workspace and Project Organization**

- A Workspace is a top-level tenant, such as an organization or company. It groups multiple related Projects, which represent distinct configurations, datasets, or environments. For example, a company might have a project for internal documentation and another for support ticket summarization.

- All data — embeddings, original content, tool usage, etc. — is scoped to a project, and each project belongs to one workspace. This enables isolation, fine-grained configuration, and per-project model/version choices.

### **Storage Model Using LlamaIndex**

Instead of maintaining a separate datasets abstraction (which risks duplicating what LlamaIndex already models), we will adopt LlamaIndex’s native data model for storage and retrieval. LlamaIndex internally organizes vectorized content using the following database schema:

- documents table – Stores raw, original documents (e.g. text, titles, metadata).

- nodes table – Stores content chunks derived from documents. Each node typically corresponds to a semantically meaningful piece of text (a paragraph, sentence, etc.).

- embeddings table – Stores vector embeddings associated with nodes. Each row includes the node ID, embedding vector, and optionally, metadata or labels.

- index_metadata or index_struct – Stores structural and config info about indexes (e.g., index type, model used, etc.).

These tables are created and managed by LlamaIndex using its SQLDatabase or PGVectorIndex classes. The benefit of using this model is that it already captures:

- Document provenance (via document_id)
- Relationships between documents, chunks, and embeddings
- Metadata-driven filtering and retrieval
- Embedding model configuration, versioning, and vector search

In our platform, each project will map to a LlamaIndex index with its own logical namespace. If using the Postgres + PGVector integration, each project can either:

- Use shared tables with a project_id column added to the LlamaIndex schema (requires extending LlamaIndex’s default schema behavior).

Given the requirement that all data is project-scoped, we will implement the **Separate Table per Project** approach, creating completely separate tables (and indexes) for each project's vectors. This method ensures data isolation and security by design and allows each table to be tailored to the project's chosen embedding model, since each table's vector column is defined separately.

Alternative approaches, documented below, include using a **Single Table with a Project ID** filter and a **Partitioned Table by Project** approach. The Single Table approach uses the project_id in the WHERE clause for every query and enforces it at the DB level with Row-Level Security policies. The Partitioned Table approach uses Postgres table partitioning to create a partition of the vectors table per project.

Given the requirement that all data is project-scoped, either of these strategies will work. If the number of projects is relatively small and data volume per project is large, partitioning is attractive. If the number of projects is very large but each has smaller data, the single table + RLS approach might be simpler.

**Indexing and Performance:** We use PGVector’s indexing to speed up similarity search. PGVector supports approximate indexes; in this design we used the HNSW index (USING hnsw) for the embedding column. HNSW (Hierarchical Navigable Small World) is a graph-based ANN index that gives fast recall for high-dimensional vectors. We choose HNSW because it supports dynamic updates (inserts/deletes) reasonably well and does not require full reindexing on data changes (unlike the IVF flat index which can become stale with data drift) . This is important considering we have dynamic data updates; we want the index to handle new or changing vectors without a complete rebuild. We will need to tune HNSW parameters (M, ef) for our use case, balancing recall vs. performance. For smaller projects or development, we could also use a brute-force (flat) index or no index (which means a sequential scan with distance computation) – but as data scales up, HNSW is recommended for production.

We also create a GIN index on the metadata JSONB (with jsonb*path_ops or the default, which indexes every key and value). This index is used to quickly filter by labels. For example, a query with WHERE metadata @> '{"user_id": "123"}' can use this GIN index to find all rows that have user_id = 123 without scanning the whole table. Using the containment operator (@>) on JSONB is a convenient way to check for a key/value presence. We can combine multiple conditions in one JSON containment (e.g. metadata @> '{"user_id": "123", "vm_status": "active"}' checks both labels at once). The GIN index will index the presence of each key-value pair, making such multi-label filters efficient. However, it’s worth noting that when combined with a vector similarity search, Postgres might not use the GIN index \_and* the HNSW index simultaneously in a straightforward way – it will typically use one index (often the HNSW for nearest-neighbors) and then apply the other condition as a filter . To mitigate this, if certain label filters are almost always used (like project_id or dataset_id), those could be part of a **partial index** or the table’s partitioning as mentioned. For instance, each project partition only contains one project’s data, so the HNSW index on that partition implicitly only searches that project. For other labels like vm_status, if it’s a low-cardinality field (say just “active” vs “inactive”), one could even maintain separate indexes or use a partial index like CREATE INDEX ON vectors USING hnsw (embedding) WHERE metadata->>'vm_status'='active'; to optimize queries that filter by active items. In general, we aim to keep label filtering flexible and rely on the combination of HNSW + post-filtering for correctness, with the option to optimize via partitioning or additional indexes if query patterns demand it .

The **vectors table schema** should also include timestamps or version info (created_at, updated_at) to manage re-indexing. For example, if we re-embed all data with a new model, we might want to keep track of which embedding version each vector is (perhaps a embedding_model_version label in metadata). Alternatively, we could maintain separate vector tables for different embedding models or a column indicating the model. However, this might be beyond the immediate scope – typically you’d update in place or rebuild the index in a migration scenario.

**Example Workflows**

To illustrate how this design works in practice, here are a few typical workflows:

- **Ingesting a Static Document:** A user uploads a PDF manual to the “ProductDocs” dataset in their project. The system splits the PDF text into chunks (e.g. by paragraphs or sections). For each chunk, it creates a new row in the vectors table with that chunk’s text, the embedding vector, and metadata. The metadata might include {"dataset": "ProductDocs", "document_id": "manual123", "file_name": "GadgetManual.pdf"}. The project_id is set to the user’s project and the dataset_id to “ProductDocs”. All these chunks are now searchable via similarity queries. If later the user updates the document, the system can locate all vectors with document_id = manual123 and update or remove them before inserting new vectors corresponding to the updated content.

- **Syncing Dynamic Data (External Service):** A project has a dynamic dataset “CloudVMs”. A background job runs every hour (or the user triggers a sync) to update this dataset. The job calls the cloud provider API to list all VMs owned by this project’s workspace. Suppose it finds a VM with ID “vm-abc123” that is running. It constructs a text description (e.g. “VM abc123: Ubuntu 20.04, 4 CPU, 8GB RAM – Status: active in us-west-2 region.”) and generates an embedding for this text. It then upserts into the vectors table: if an entry for “vm-abc123” exists, it updates the embedding, content, and metadata (say the status changed from inactive to active, the metadata vm_status is updated to “active”); if it doesn’t exist, it inserts a new row with metadata: {"dataset": "CloudVMs", "vm_id": "vm-abc123", "vm_status": "active", "region": "us-west-2"}. If some VMs were terminated, their corresponding rows might be deleted or marked with vm_status = "terminated" (so they can be filtered out). Now, when a user asks “How many VMs do I have running?”, the system can formulate a vector query (maybe using a query embedding for the question) and add a filter dataset = CloudVMs AND vm_status = active. The results returned will be chunks describing the active VMs, which the system can then post-process to count or summarize.

- **Query with Label Filtering:** A user in project A (who is tied to user_id = 123 in that project’s data) asks a question that should only retrieve their own information. The application knows the query is user-scoped, so it executes a similarity search with a filter metadata->>'user_id' = '123'. Internally, PGVector finds the nearest neighbor embeddings and Postgres filters out any chunks not labeled with that user_id. The final results (e.g. text from documents that user 123 created, or data items belonging to them) are returned. In another scenario, an admin user might query across the whole project without a user filter – in that case, the filter might be omitted or different. The label system is flexible: you could combine filters like user_id = 123 AND confidential != true to e.g. exclude any chunks labeled as confidential in results shown to a certain role.

- **Re-indexing with a New Model:** The platform decides to upgrade the embedding model from v1 to v2 for better accuracy. To re-index a project’s data, it can iterate dataset by dataset (or directly all rows in vectors). For each stored chunk content, it feeds it to the new model to get a new embedding vector. It then updates the embedding field in the table (or optionally writes to a new table/version to allow A/B testing). Because the original data is readily available in the content field, this process can be automated. During this, the system might also update a label like embedding_model: v2 in the metadata to mark the version. The result is that all vectors are now refreshed. The HNSW index may need to be rebuilt or updated – PGVector’s HNSW can handle incremental updates, but a full rebuild might be done for optimal performance (this could be done in a transaction or by building a parallel index and swapping, to avoid downtime). The dataset concept helps here: the system could choose to re-index one dataset at a time to manage load (e.g. re-index dynamic data more frequently than static, or vice versa).
   **Using Dataset as a Filter or Context:** If a project has multiple datasets, users might direct queries to specific ones. For example, “Search in SupportTickets for error code 500.” The application can translate that to adding a filter dataset = SupportTickets so that only ticket data is searched. Alternatively, if no specific dataset is mentioned, the default could be to search across all datasets in the project (which the vector query will do if no dataset filter is applied). Because each vector knows its dataset (via a label or dataset_id), the user could also get an understanding of where each answer came from (e.g. the UI can show “this answer is from dataset: ProductDocs, document: GadgetManual.pdf”). This traceability is a direct benefit of having structured metadata stored with the vectors.

In all these workflows, the combination of **project scoping**, **datasets**, **stored original content**, and **labels** provides a robust and flexible way to manage a multi-tenant vector database. Projects keep tenants fully isolated, datasets group related data and preserve original texts, and labels allow rich metadata to travel with each chunk from ingestion to query. This design supports efficient re-indexing and filtering, ensuring that as the platform scales to many projects and diverse data sources, it remains manageable and performant. All metadata filtering happens within the context of a project, keeping queries efficient and relevant. By leveraging PostgreSQL’s capabilities (JSONB indexing, foreign keys, partitioning, etc.) alongside PGVector’s similarity search, we create a solution that is both **practical** (easy to implement with SQL) and **scalable** (capable of slicing and dicing the vector search space on demand for each tenant’s needs).

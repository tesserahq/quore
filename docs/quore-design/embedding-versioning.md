## Embedding Versioning in a Multi-Tenant LLM Platform

## **Schema Design Strategies**

**Challenge:** The platform needs to track which embedding model/version generated each vector, to avoid mismatched comparisons (e.g. querying with a new model’s embedding against old model vectors yields meaningless results ). PGVector stores fixed-length vectors, so mixing embeddings of different dimensionality in one column is problematic . We outline three schema approaches to support versioning:

### **Single Table with a Version Column**

**Description:** Use one shared embeddings table for all projects, adding a column (e.g. embedding_version or model_id) to identify the embedding model/version for each vector . All embeddings (across versions and projects) reside in this table, each row tagged with its version.

- **Pros:** Keeps schema simple (one table to manage). Easy to add a new version: just insert new rows with a new version tag. Metadata (like document text or IDs) can be shared, avoiding duplication – e.g. store document content in a separate table and have multiple vector entries referencing the same doc ID with different versions. Querying the latest version is straightforward by filtering on the version column. If needed, you can compare versions by querying different version filters within the same table (though comparing embedding vectors directly is non-trivial ).

- **Cons:** **Query complexity:** Every vector search query must include a filter for the correct version and project, or else risk mixing tenants/versions. This adds overhead and potential for bugs if a filter is missed. **Indexing:** PGVector indexes (IVFFlat, HNSW) are built on a specific vector column. Mixing versions can degrade performance unless queries always filter by version (the index would still scan irrelevant vectors if not filtered). We can create **partial indexes** per version (or per project) to mitigate this – for example, an index on the vector column with a condition on embedding_version – but that adds operational overhead (creating a new index whenever a new version is introduced). **Vector dimensionality:** The vector column’s type must accommodate all versions. PGVector typically requires a fixed length (e.g. VECTOR(1536)), so this approach works best if all embedding versions have the same dimension. If a new model has a different length, the table would need a new column or to be rebuilt. A workaround is to define the column as VECTOR with no fixed length (PGVector supports a generic vector type) and store vectors of varying size . However, this requires filtering by vector_dims(embedding) or similar to avoid runtime errors, and separate indexes for each dimensionality (e.g. an index per version or a numeric dims column to filter on ). Overall, a single table maximizes shared infrastructure but can complicate queries and indexing when multiple versions or dimensions come into play.

- **Multi-Tenancy:** With a shared table, a **project_id** column is also needed to isolate data per tenant. Row-Level Security (RLS) could enforce that queries only see their project’s data . Versioning intersects with project isolation in that each project might use a different embedding model. In this schema, the combination of project_id and embedding_version would segregate vectors (e.g. Project A’s version 1 vs. Project B’s version 2). This allows different projects to concurrently use different embedding models, but all still reside in one physical table. Administrators must ensure that both project_id and version filters are applied in vector queries to prevent any cross-tenant or cross-version search.

### **Separate Tables per Embedding Version**

**Description:** Maintain separate PGVector tables for each embedding model or version. For example, an embeddings_v1 table and an embeddings_v2 table (and so on) for each project or model. When a project upgrades to a new embedding model, a new table for that version is created and populated with that project’s data.

- **Pros:** **Isolated indexes:** Each table has its own vector column and index tuned to the specific model. This cleanly handles different vector dimensions (each table’s column can be defined with the correct length for that embedding model). There’s no risk of querying across versions by accident because the data lives in different tables (similar to how Weaviate requires a new class per embedding model ). This approach simplifies rollback: the old version’s table remains intact, so reverting is as easy as switching back to querying the old table. It also simplifies operations like dropping an outdated embedding set – you can DROP TABLE for an old version once no longer needed, freeing storage. During **A/B testing**, two tables (old vs new) can operate in parallel (both indexes loaded in memory separately), and traffic can be split between them easily by choosing which table to query. From an operational standpoint, building a fresh index in a new table won’t interfere with the live queries on the old table. Each table can be created in the project’s schema (if using schema-per-tenant) or carry the project name in the table name to keep tenant data separate.

- **Cons:** **Storage duplication:** Vectors (and possibly metadata) are duplicated across tables for each version, increasing storage use. If a project has N versions stored, the same document might have N vector copies. There is also overhead in managing many tables, especially in a multi-tenant environment – potentially #projects × #versions tables. This can become unwieldy if not automated, and database management tasks (migrations, permissions, etc.) must workspace for dynamic table creation. **Query routing:** The application must know which table to use for a given project’s query. This likely means keeping a mapping of “active embedding version per project” in a config table. The code or LangChain/LlamaIndex integration would then choose the appropriate table (vector store) at runtime. While separate tables isolate versions, they also make it harder to do a combined search (if that ever mattered) or to maintain a single code path – you might need conditional logic for which table to query. Additionally, if many small projects each have their own tables, you lose some benefits of shared indexing – though PG is efficient at handling many tables, very large numbers could impact planning or memory.

- **Multi-Tenancy:** This approach naturally supports project isolation. We could either have global version tables that include a project_id (less ideal, mixes tenants) or, more cleanly, separate **per-project, per-version** tables. For example, project123_embeddings_v2 for project 123’s version 2 data. This ensures no cross-tenant data in the same table (strong isolation). However, it multiplies the number of tables. Another variant is to use **schemas per project** – e.g. each project has its own schema with an embeddings table, and when upgrading, the old data might be moved to a backup table in that schema. That still results in multiple tables per project, but at least they’re namespaced by schema. In any case, multi-tenancy concerns are mostly addressed by design (each project only queries its own tables). The trade-off is operational complexity in keeping track of many tables/schemas.

### **Partitioned Tables by Version or Project**

**Description:** Use PostgreSQL table partitioning to split data by embedding version and/or project. For instance, a single logical table embeddings can be partitioned by project_id (each tenant’s data in a separate partition) or by embedding_version, or a combination (sub-partitions by project then version). Each partition is essentially a separate physical table behind the scenes.

- **Pros:** Partitioning gives many of the benefits of separate tables while retaining a unified logical schema. If partitioned by version, each embedding version’s data lives in its own partition – you can build a dedicated index on each partition, appropriate to that model’s dimension. Queries filtering by version automatically get routed to the correct partition (partition pruning), so query performance is similar to hitting a separate table, but the application can still query the top-level table. Likewise, partitioning by project isolates tenants’ data; queries specifying a project get partition pruning, improving performance and safety (and you can combine project+version as partition keys for a two-level partitioning). **Operationally**, adding a new version can be as simple as adding a new partition (no need for a wholly new table definition beyond the partition itself). Dropping a partition when retiring a version is quick. This also scales better management-wise than hundreds of separate free-floating tables – you still have many partitions, but they are administered under one table context. Partitioning is a good middle-ground for multi-tenancy: for example, each project could be one partition (if each project only ever uses one embedding model at a time, partition-by-project might suffice; if projects can have multiple versions active, sub-partition by version within project might be used).

- **Cons:** Partitioning adds complexity to the database setup and requires careful planning of keys. All partitions of a table share the same columns, which means if using the fixed-dimension vector type, you’d still need a consistent vector length across all partitions of that table. In practice, you might define the vector column as the largest dimension you expect (and pad smaller vectors with zeros) or use the generic vector type with a check constraint per partition to enforce the expected length. The former (padding) wastes space and could skew distance calculations if not careful (trailing zeros in smaller vectors might not affect cosine similarity too much, but mixing embeddings from different models is conceptually unsound to compare directly ). The latter (generic vector + constraint) is more flexible but complicates index creation (each partition would have its own index tuned to its dimension, and queries must still avoid crossing partitions unintentionally). **Operationally**, partition management is an added burden – for each new version or new project you must create partitions (though this can be automated). Also, certain PGVector index types might not support global index across partitions, so each partition is indexed separately and the query planner has to search relevant partitions’ indexes. This is usually fine, but it’s not a single index over all data (nor would we want one in this case). Debugging and monitoring can be trickier with partitions, as you have to check each partition’s index usage, size, etc. In summary, partitioning can enforce project and version isolation at the database level (even allowing the database to ensure no cross-version search if the partition key is version), but it requires more upfront work and careful schema design.

- **Multi-Tenancy:** Partitioning naturally encodes tenant isolation if using project_id as a partition key. For example, a partition per project means no indexes or data are shared across tenants, avoiding one tenant’s large index scans affecting another. Versioning can be integrated by having partitions per (project, version) pair. This achieves a similar isolation to separate tables per project-version, but within a structured framework. One thing to note is that with many tenants and versions, the number of partitions could grow large; Postgres can handle many partitions (especially with Postgres 15+ improvements), but extremely high counts might impact performance. Thus, this strategy should be used if the scale is manageable or if old partitions can be dropped regularly.

**Trade-off Summary:** In choosing a schema, we balance **query simplicity** (fewer moving parts vs. need to always filter correctly), **storage vs. speed** (duplicate vectors to isolate versions vs. store once and filter), and **operational complexity**. A single table with a version column is simplest initially but demands strict query discipline and doesn’t handle varying dimensions well . Separate tables give maximum flexibility (different schema per model and clean isolation ) at the cost of more metadata management. Partitioning offers a structured compromise, though it may be an over-engineered solution unless we anticipate frequent version overlaps or want to leverage built-in partition pruning. Regardless of schema, we must ensure the design meshes with project isolation: each query should only search that project’s vectors of the appropriate version. Storing the embedding model name or version alongside each vector (as a column or implicit in table name) is critical to avoid the situation where we lose track of which model produced an embedding .

## **Operational Workflows**

Introducing embedding versioning impacts how we maintain and update the vector store over time. We need well-defined workflows for re-indexing data, testing new models, and rolling back if needed, all while considering PGVector index maintenance.

### **Re-indexing When Upgrading Embeddings**

When adopting a new embedding model version for a project, all existing source data must be re-embedded and indexed using the new model. Embeddings from different models are not comparable, so **you cannot mix old and new vectors in search results** – essentially, a full re-index is required . The workflow might be:

1. **Offline Embedding Generation:** Take the project’s documents or data and generate embeddings with the new model version (e.g. using LangChain or LlamaIndex to batch-process documents with the new embedder). This can be done in the background to avoid disrupting the live system.

2. **Bulk Insertion:** Insert the new embeddings into the vector store. Depending on the chosen schema, this could mean adding new rows with embedding_version = X in the shared table, or populating a new version-specific table/partition. Bulk loading should be done carefully to manage transaction size and not bloat the database. If using ivfflat or HNSW indexes, it may be faster to disable or delay index creation during the bulk insert and build the index after all inserts are done (PGVector supports creating an index after data load, which can be faster than incrementally indexing each insert).

3. **Index Building:** Ensure a vector index is built for the new embeddings. For a shared table approach, if using a single index on the vector column, it might already cover the new data (though query performance might be suboptimal without filtering by version). More likely, we’d create a new index (or a new partition’s index) specific to the new version’s vectors. Building an index on a large embedding set can be time-consuming; however, PGVector has improved index build performance (e.g. parallel index build) to handle large volumes . This step should also be done offline if possible (the old version remains in use while the new index builds). With separate tables, building the index on the new table doesn’t impact the old table’s performance.

4. **Validation:** Before switching over, validate the new embeddings. This could involve running a sample of queries against the new index (in a staging environment or with internal tests) and comparing results with the old version. This is important because embedding model changes can alter retrieval quality. As LlamaIndex guidelines suggest, one should _“evaluate the impact of updating to newer versions”_ of embeddings . We might measure if the new version returns relevant results as expected, and possibly measure some metric of success (e.g. downstream QA accuracy).

5. **Deployment/Cut-over:** Once new embeddings are fully indexed and validated, update the application to use them for live queries. For a single-table scheme, this could mean flipping a “current embedding version” flag for the project (so queries now filter on the new version). For separate tables, it means routing queries from embeddings_v1 to embeddings_v2. This cut-over should be done carefully, possibly during low-traffic periods. If the system allows per-query selection, we might initially route only a percentage of queries to the new version (see A/B testing below) before full cut-over.

Throughout re-indexing, **consistency** is key: the system should not mix old and new vectors for the same query. Typically, the query embedding itself must be generated by the same model version as the vectors it’s compared against. If the user’s query is embedded with version 2 but hits version 1 vectors, the similarity scores are invalid . Our workflow must ensure that once we switch a project to version 2, the query embedding function is also updated to use version 2, in sync with the data.

### **A/B Testing New Embedding Models**

Before fully committing to a new embedding model, we may want to run an A/B test to compare its performance against the current version. This can be done by maintaining **dual indexes**: keep the existing embeddings (version A) and introduce the new embeddings (version B) in parallel. Strategies for A/B testing include:

- **Traffic Splitting:** Route a percentage of user queries to use the new embeddings while the rest use the old. For example, 10% of queries (or specific users/projects) are answered using the version B index (by embedding the query with model B and searching version B vectors), while 90% use version A as usual. Collect metrics on both sets – user feedback, click-through, or qualitative evaluation of results. Over a trial period, analyze whether version B provides better semantic matches or downstream LLM responses. This live experiment is valuable because it uses real queries.

- **Shadow Testing:** Alternatively, every query could be run on both versions in parallel (the user sees result from A, but we silently also fetch from B and compare outcomes). This doubles the query load (might be expensive), but provides one-to-one comparisons on identical queries. We would log differences, maybe have evaluators rate which result was better, or check if the LLM’s answer improved with the B retrieval. This approach isolates the evaluation to backend without affecting users.

- **Dual Retrieval Fusion:** In some cases, you might even return results from both indexes to the LLM (though mixing context embeddings from different models is tricky). More often, A/B is done separately, not merging results.

To support A/B testing operationally, our system must handle two embedding versions simultaneously. This means storing both sets of vectors (as discussed in schema strategies) and being able to perform queries on either. Using separate tables for A and B is straightforward – you have two vector stores, and the application chooses one or the other per request. If using one table with versions, you can query it with a WHERE embedding_version = A or B accordingly (or call a SQL function that takes a version param to route to the right subset). **LangChain** or **LlamaIndex** can be configured with different embedding models and vector store instances; we could instantiate two retrievers – one with the old embedder+index, one with the new – and switch between them in code or run them both.

During A/B tests, we should closely monitor performance and accuracy. We might use metrics like MRR (Mean Reciprocal Rank) or precision@k if we have labeled relevant documents, or downstream metrics (did the LLM’s answer use correct info?). We should also monitor query latency – if the new model has higher dimensional vectors or the index is larger, does it slow down searches? These factors feed into the decision to roll out the new version.

One **operational consideration** is the load on PGVector: maintaining two full indexes (old and new) will use more memory and disk IO for searches. PGVector’s index types (like IVF) might hold data in memory proportional to dataset size; doubling it could impact DB memory usage. We should ensure the database can handle the combined load during the test or mitigate by testing on a subset of data if needed.

### **Rollback Mechanisms**

If a newer embedding version underperforms – for example, search relevance drops or it introduces latency – we need the ability to quickly rollback to the previous version. The design should facilitate rollback with minimal downtime:

- **Retention of Old Embeddings:** Never immediately delete or overwrite the old vectors when introducing a new version. Instead, keep the old index intact until the new version is proven. This is inherent in both the single-table and multi-table approaches: in a single table, we’d keep the old entries (version N-1) and just mark them inactive; in separate tables, we simply leave the old table in place. This way, rollback is simply a configuration change, not a re-computation. For instance, if we switched a project’s “active embedding version” setting to 2, to rollback we set it back to 1 and direct queries to version 1 data again. The switchover logic could even be toggled via feature flag for quick action.

- **Automated Rollback Triggers:** We might integrate monitoring that triggers a rollback if certain conditions fail. For example, if during a gradual rollout the new version’s metrics fall below a threshold (say user satisfaction or an evaluation score), the system could auto-revert to the last known good version. Even without full automation, having a runbook for rollback (steps to point the app back to the old embeddings) is crucial.

- **Data Consistency on Rollback:** If any new data (documents) were ingested while the new version was live, those documents won’t have old-version embeddings. This is a complication: if we revert, those new documents wouldn’t be searchable until we embed them with the old model. One way to handle this is to, during the new version rollout, _also_ embed any new documents with the old model as a backup. That way, both indexes remain up-to-date. This doubles the embedding cost temporarily, but ensures no gap in coverage. If that’s too costly, an alternative is to accept that a rollback might require time to backfill embeddings for new docs added in the interim (or simply mark those as not searchable until re-embedded).

- **Rolling Back Index Changes:** In PGVector, if we created a new table or partition for version 2, rollback just means switching the query target, no DB schema change needed (the new table can be left in place or dropped later). If using a single table with version flags, rollback means adjusting queries to use the old embedding*version. This is straightforward if the data was untouched; however, if we had tried to \_replace* the embeddings in-place (which we generally avoid for this reason), rolling back would require re-inserting old vectors. Thus, the safer approach is additive: add new embeddings in parallel rather than destructive replacement. The Arize AI example highlights this scenario: an iteration of an embedding model that seemed promising ended up worse on a larger scale, and the need arose to “go back to your best version” . By planning for rollback, we ensure we _can_ go back without starting from scratch.

In summary, a robust versioning design treats each embedding model upgrade similar to a code deployment: keep the last stable state, try the new one, and be ready to revert quickly. Since embedding generation can be expensive, preserving previous vectors saves significant time if a rollback is needed.

### **Impact on PGVector Indexes and Performance**

Embedding versioning can affect how we use PGVector indexes. Key considerations include:

- **Index per Version:** As discussed, it’s generally wise to maintain a separate index for each embedding version (whether via separate tables or partial indexes). This ensures that index structures (like the IVF centroids or HNSW graphs) are optimized on a homogeneous vector set. Searching in an index that contains mixed embeddings would not yield meaningful results, and index accuracy could suffer. If using the IVFFLAT index type, for example, the clustering (centroids) should ideally be built on vectors from one distribution. Combining different models’ vectors (even of the same dimension) in one index could make those centroids less effective. Thus, we expect to have one index _per model version per project_. This increases the number of indexes stored, but each index is only queried when that model is in use.

- **Index Build Times:** Introducing a new version means building a new index from scratch. For large datasets, this can be time-consuming (hours, potentially). Planning for this, we might allocate maintenance windows or do index builds on a replica or offline database. PGVector’s performance is continually improving – for instance, Neon’s team added a parallel index build that significantly speeds up constructing indexes . We should stay updated on such features to reduce downtime. If downtime is a concern, another approach is to initially use **non-indexed search** (brute force) for the new embeddings until the index is ready. This is slower for large sets, but if the new version is only gradually rolled out, it might handle the lower query volume initially. Once the index build completes, we switch fully.

- **Index Storage and Maintenance:** Multiple versions mean more index storage. PGVector indexes (especially HNSW) can be large. We need to monitor disk usage; if an old version is truly retired, dropping its index/table frees space. Routine **VACUUM** and **ANALYZE** are important after bulk inserting new vectors, to update table statistics (especially if using scalar filters alongside vector search). The Postgres planner uses these stats to decide when to use indexes. With partial indexes or partitions, an ANALYZE ensures the planner knows, for example, that if you filter on version = 2, it should use the version 2 index.

- **Vector Dimension Changes:** If a new embedding model has a larger vector size, the index might also be larger and slower. For instance, going from 1536-d to 3072-d doubles the data per vector. This can increase distance computation cost and memory usage per index entry. PGVector supports up to a certain dimension (there are limits depending on index types – e.g. IVF with 1536-d is fine, but extremely high dims might hit performance limits ). In extreme cases, we might consider **dimensionality reduction** or **quantization** for very large vectors, but that’s beyond versioning scope, except to note that switching to a model with drastically higher dimension could force infrastructure changes (like more memory or switching index type).

- **Project Isolation in Indexes:** If using a single shared table index for multiple projects, one project’s heavy indexing or re-indexing could interfere with another’s query performance (because they share the table and possibly the index if not partitioned). This is another argument for either separate indexes (partial or table) per project or using partitioning. The Salesforce engineering post on multi-tenant AI workloads notes the importance of isolating heavy operations to avoid noisy neighbor issues . In practice, building an index for project B’s new embedding version should ideally not lock or slow down project A’s queries. By using separate indexes/partitions, we contain such maintenance overhead within the tenant that needs it.

In conclusion, operational planning for embedding versioning involves carefully **managing re-embedding jobs**, conducting safe **A/B tests**, and maintaining the **indexes** so that each version serves queries efficiently. The system should be designed to make these tasks as smooth as possible – for example, by providing tools to re-index data for a new model and switch versions, akin to how some vector databases offer versioning features (DataRobot’s vector DB versions allow adding or replacing data sources for a new version ). Our design, leveraging Postgres + PGVector, will implement these capabilities through our own schema and application logic.

## **Query Routing and Runtime Selection**

With multiple embedding versions potentially in play, the platform must decide **at query time** which embeddings to use for retrieval. This involves routing the query to the correct vector index and ensuring the query embedding itself is generated by the corresponding model.

### **Query-Time Version Selection**

By default, a project will use its **latest embedding version** for all queries after an upgrade. This implies that the system keeps track of an active version per project (e.g., in a project config table: project_id -> current_embedding_version). The retrieval workflow would then:

1. Look up the project’s current embedding model/version.

2. Use the appropriate embedding model to encode the user’s query into a vector.

3. Search the PGVector store for that project, filtered to the matching version (or directed to the corresponding table/partition).

This ensures consistency between query vector and index . However, we might have cases where a different selection is needed:

- **User-Specific or Session-Specific Version:** Perhaps during a phased rollout or A/B test, certain users are pinned to the old version vs. new. In that case, the query routing might override the default and choose a different version for that query. The system could accept a parameter (possibly hidden from end-users, controlled by feature flags or experiment frameworks) to specify which version to use for retrieval. For example, an internal flag like use_embedding_version=X that forces the query embedding and search to use version X. This would allow concurrent usage of different versions within the same project for testing or gradual migration.

- **Model-Matching:** In some scenarios, the embedding version might be tied to the language model used to answer the question. For instance, if one embedding model is known to work better with a certain LLM (or if using a proprietary embedder when querying a specialized model), the system could choose an embedding version based on context. More straightforward is matching the model name: if the platform allows the user to select an embedding model on the fly (say, project config says “use model A” or “use model B” for this query), we route accordingly. LangChain and LlamaIndex both allow specifying which embedding function to use for a query; we need to hook our routing logic into those layers.

- **Fallback Logic:** In theory, if a query returns poor results with the new version, the system could try querying the old version as a fallback (this would be a dynamic rollback per query). This is advanced behavior and would require careful handling (to not double-answer the user or degrade performance). It’s more likely something we’d do in an evaluation context than in production user flow.

Implementing query routing typically means our **retriever component** (in LangChain/LlamaIndex) isn’t hard-coded to a single embedding index. Instead, it might be a custom retriever that checks the project’s setting and then dispatches to one of multiple vector stores. For example, we could maintain a dictionary of vector store objects keyed by (project, version). On each query, pick the right one. Alternatively, if using direct SQL, we can parameterize the SQL to include the version filter. In LangChain’s PGVector wrapper, one might instantiate it with a SQL filter or a different table name per version.

### **Per-Query vs. Per-Project Configuration**

**Per-Project (Default):** The simplest configuration is one embedding model per project at any given time. This means all queries for that project use the same index. It simplifies mental models and usually, this is what we want (consistent user experience). In the design document context, we assume _embedding model flexibility per project_ – i.e., each project can choose its model, but does not normally mix them. So the standard mode is per-project version setting.

**Per-Query (Override):** Nonetheless, having the ability to override on a per-query basis is useful for experimentation and possibly for advanced users. If we expose it, we must ensure that the query embedding and search are both switched in tandem. This could be exposed at an API level as an optional parameter like ?embedding_model=bert_large (if we want external control). Alternatively, it’s hidden and only used internally for testing (more likely, since average users shouldn’t worry about embedding versions).

Allowing per-query choice raises the question: could a user intentionally query across different versions? Generally, **mixing is discouraged** – you wouldn’t want to search version 1 and version 2 simultaneously and combine results, because the similarity scores aren’t comparable. If someone insisted on merging results from two versions, we would have to normalize scores or treat them separately. It’s safer to avoid this scenario. Instead, if a comparison is needed, one could run two queries (one per version) and then manually reconcile results (for analysis purposes). But in production, stick to one version per query.

**Administrative Control:** There might also be an admin interface where a project owner can trigger an upgrade or choose a version. For example, after re-indexing with a new model, the owner might flip a switch “Use new embedding model”. That would update the per-project setting and subsequent queries use that version. If they encounter issues, they might flip back. This is essentially manual project-level version selection, which the system should support.

### **Handling Different Vector Dimensions at Query Time**

One important runtime consideration is when embedding models differ in vector dimension. Since PGVector requires the query vector to match the index vector dimension, the system must route queries to an index of the same dimension. If a mismatch occurs, the query will error (or the distance computation is invalid). Our design ensures this by coupling the embedder with the index choice.

For instance, if Project X’s active model is text-embedding-ada-002 (1536-dimension), the PGVector index is built for 1536-d vectors. If someone accidentally tried to use a 1024-d model to embed a query and search in that index, PGVector would likely throw an error about dimension mismatch. Similarly, if using the generic vector type without fixed length, a query must filter to only rows of the same vector_dims. We can enforce this by always adding AND vector_dims(embedding) = D (or storing an embedding_dim attribute) in the search query . But ideally, our logic prevents the wrong combination altogether.

When switching models, if the dimension changes, the application configuration must also update the expected dimension in queries or function definitions. For example, if we have a SQL function match_embeddings(query_vec VECTOR(1536), ...) , a new model with 768 dimensions would require a new function (or a generic one using VECTOR type) to handle that length. In practice, using a parameterized approach (like building the query string with the vector literal) or migrating to a polymorphic function could solve that. It’s a reminder that embedding dimension is a kind of **schema version** itself.

**Multiple Vector Columns per Table:** If, for some reason, we had multiple embedding columns (say embedding*v1 and embedding_v2 in one table), the query would need to choose the correct column to compare against. That is another form of runtime selection. This approach (multiple columns) is generally not favored because adding a new column for each version changes the schema frequently and results in sparse data (only one column filled per row) . It’s simpler to use separate rows or tables. Therefore, our query routing will typically select \_which dataset (table/partition) to query*, not which column.

**Edge Cases:** If an LLM query is routed to the wrong embedding version by mistake, the user might get either no results (if the similarity threshold isn’t met due to mismatched vectors) or nonsensical results. This reinforces that the system should have safeguards. For example, each project’s query pipeline could carry the embedding_version context so that by the time the vector search is executed, it’s guaranteed to be correct. This could be implemented by tying the embedder and the vectorstore together in a higher-level object (as LlamaIndex does in its indices: the index knows which embedding model was used). In fact, one of the lessons from LlamaIndex is that it currently doesn’t store which model was used in the index, leading to potential confusion . Our platform should improve on that by always associating embeddings with their model version explicitly, and using that info at query time.

### **Configuration Examples**

To illustrate, suppose Project Alpha initially uses EmbeddingModel1 (version 1, 768-dim). All its data is in embeddings table with project_id=Alpha, version=1. Later, we support EmbeddingModel2 (version 2, 1024-dim). We re-embed all of Alpha’s docs with model2 and store them (perhaps in the same table with version=2). Now, in the project settings, we mark current_version = 2. The query flow for Alpha becomes:

- The system sees project Alpha, current_version 2.

- It loads/chooses the embedder for model2 (maybe through LangChain’s embedding class).

- The user’s question is embedded into a 1024-length vector.

- The vector search query executes with ... WHERE project_id = 'Alpha' AND embedding_version = 2 ORDER BY embedding &lt;-&gt; $query_vec LIMIT k (or hits the partition/table for Alpha_v2).

- Results come back, and then we proceed to use them in the LLM response.

If a developer or admin wants to test version 1 on a specific query, they could override embedding_version=1. Then the system would: use model1 to embed the query (768-dim), and query the version=1 index. This would retrieve legacy results for comparison.

We also must consider **cross-project queries** (if any). Usually, each project’s data is separate. But if we had a scenario of searching a global knowledge base plus project-specific data, those would likely be separate queries merged at the application layer (since different embedding spaces can’t be directly combined). In multi-tenant systems, it’s rare to search across projects unless there’s a shared public dataset – even then, that shared data would have its own known embedding model.

Finally, **compatibility with LangChain/LlamaIndex:** These frameworks typically treat the vector store + embedding function as a bundle. For example, a LangChain VectorStoreRetriever might have an embedding_function. If we integrate versioning, we might subclass or wrap these to inject our logic. The design document doesn’t need code, but it’s important to note that maintaining the correspondence between the embedding function and the index is crucial. If we do it right, the user (or the rest of the system) can just call a high-level retrieval API and under the hood, the correct embedding model and index are chosen, transparently.

In summary, **runtime selection** ensures that each query is answered using the appropriate embedding version for the project and use-case, with the option for controlled overrides. The system’s routing logic, guided by project settings and perhaps experiment flags, will prevent mismatches and allow smooth transitions between embedding model versions. By considering differences like vector dimensions and maintaining mapping of versions, we can confidently support evolving embedding models in a multi-tenant environment without breaking semantic search functionality.

**Sources:** The strategies above are informed by community experiences and best practices. For instance, developers have noted the need to re-embed all data when switching models and to explicitly store embedding version metadata . Systems like Weaviate currently support only one embedding per object, requiring manual handling of versioned classes – our design workspaces for that by planning our own version separation. Additionally, lessons from LlamaIndex highlight the importance of tracking the embedding model used . By incorporating these insights, the proposed design provides a comprehensive approach to embedding versioning using PGVector, LangChain, and LlamaIndex integration. This ensures the platform can accommodate future embedding model improvements while maintaining project isolation and search accuracy.

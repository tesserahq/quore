## Plugins

Plugins are **extensible tool integrations** that the platform can leverage to perform actions or fetch data via standardized APIs. In this design, plugins follow the **Model Context Protocol (MCP)** format – essentially each plugin is an independent MCP server (often open-sourced in a GitHub repository) exposing one or more “tools” that an AI agent can use. The platform’s plugin subsystem is inspired by **Connery**, an open-source plugin infrastructure for AI apps . Just as Connery allows developers to package actions into standalone plugin servers and handles the runtime and authorization for using them, our platform will let administrators **install plugins from GitHub repos** and make them available to LLM agents.  
   <br/>**Plugin Metadata and Structure:** Each plugin is identified by a repository (e.g. a GitHub URL). The repository contains the code for an MCP server – typically implemented either in Python (using the MCP Python SDK) or TypeScript/Node (using the MCP TypeScript SDK). The code defines a set of **Tools** (functions the LLM can call) and possibly **Resources** and **Prompts** as per the MCP spec . Tools correspond to operations with side effects or computations (e.g. “query a database”, “send an email”), while Resources are read-only data fetchers. For example, a “GitHub plugin” might expose a tool search_issues(repo, query) and a resource latest_commits(repo). Internally, the plugin code uses the MCP framework to implement these actions (calling real external APIs or databases) and exposes them via a standardized API (often an HTTP + Server-Sent Events interface, or stdio if local) to any MCP client.  
   <br/>Each plugin repository is expected to include some **metadata** describing its capabilities, such as:

- A manifest or configuration file (for instance, mcp.json or similar) listing the plugin’s name, version, description, and the tools it provides (with input/output schema for each tool).

- Alternatively, the MCP protocol allows dynamic discovery: an MCP client can connect to the server and request a list of available Tools, Resources, and Prompts . The plugin’s MCP server will respond with descriptions of each tool (names and usage). Our platform can leverage this to gather plugin metadata after installation.
  - Plugins may also include metadata like required credentials (for accessing external services – e.g. an API key for a third-party API) and might define how those credentials should be provided (environment variables, config files, etc.). Storing or managing those secrets might be handled in future (similar to how Connery manages secrets ), but at least we store what is needed and ensure the runtime can supply them.

- **Plugin Registration & Caching:** To add a new plugin, an admin of a workspace (or a system admin) will **register the plugin by providing its GitHub repository URL (and optionally a version or tag)**. Upon registration, the system will download the plugin code – e.g., clone the repository or fetch an archive – and cache it locally. This cached code is stored in a designated plugins directory and indexed in a **Plugin Registry** table in the database. The plugin registry stores metadata such as: plugin_id, repository URL, branch/commit hash of the cached version, plugin name and description, and the list of tools (capabilities) if known. By caching the plugin code, we avoid repeated network calls to GitHub and ensure a consistent version is used until explicitly updated. (In future, an update mechanism could pull a newer commit and refresh the cache upon admin request.)  
<br/>After downloading, the platform **extracts plugin metadata**. If the plugin provides a manifest file, we will parse it to record the tool definitions. If not, the platform can spin up the plugin in an isolated way to perform an MCP handshake and discovery – essentially launching the plugin server (perhaps in a subprocess or a sandbox environment) just to query its capabilities via the MCP protocol, then shutting it down. (Execution of the full plugin functionality is out of scope at this stage; we only need to discover what tools it has.) This step yields information like tool names, their input parameters, and descriptions, which we store in the plugin metadata. For example, the GitHub plugin might register tools “list_issues” and “create_issue” with certain param schemas.  
<br/>Each plugin in the registry can then be **enabled** either at the **Workspace level (globally)** or for individual **Projects**:

- _Workspace-Global Plugin:_ Enabling a plugin globally in a workspace means all projects under that workspace can use the plugin’s tools. In the database, we might have a join table linking workspace_id and plugin_id to indicate it’s enabled. The platform will then load that plugin’s tools into any agent or query associated with any project of that workspace (unless a project explicitly opts out).

- _Project-Scoped Plugin:_ A workspace may have a plugin available but not want it in every project. In this case, an admin can enable the plugin only for specific projects. We maintain a mapping of project_id to plugin_id for those cases. If a plugin is enabled at project scope, it applies only to that project’s LLM agent sessions. (A plugin could be enabled both globally and on a project; the net effect is that it’s available – the distinction is mainly for admin convenience.)

This flexibility allows, for example, a “Database Query” plugin to be enabled only for a project that deals with database Q&A, but not for another project that shouldn’t have that capability. It also allows whitelisting tools for safety (some plugins might be experimental or access sensitive actions, so an admin might restrict them to certain contexts).

**Plugin Endpoints and Performance:** Once a plugin is installed and enabled, the platform needs to know how to reach the plugin when it’s time to use it. If the plugin server is already running (executed by some runner), it will have a network endpoint (e.g. <http://localhost:12345> or a websocket or an OS pipe). We **cache the plugin’s endpoint URL/connection info** in memory or in the database, so that when an LLM agent calls a tool, we don’t incur overhead in looking up how to reach that plugin. For example, after registration, we might immediately launch the plugin server (if our architecture supports it) and note that “Plugin X is running at <http://127.0.0.1:5001”>. That address is stored in the plugin’s metadata for quick access. If we are not running it immediately (perhaps waiting for first use), we still store how to launch it and any config needed. In either case, the first time a plugin tool is invoked, the system will either already have a live connection or will start the plugin’s process and then cache the connection.

The system does **not rely on GitHub at runtime** – once cached, the plugin code is local. This means even if the GitHub repo goes offline, the installed plugin continues to function. Caching and reusing the endpoint also improve performance and responsiveness, as connecting to the plugin does not require a fresh start each time. (If needed, the platform could pool persistent connections or keep the plugin process alive between calls to avoid startup latency.)

**Plugin Integration: Discovery and Invocation of MCP Tools**

One of the key features of this platform is allowing the AI (LLM) to use plugin-provided tools to extend its capabilities. The integration follows the Model Context Protocol workflow for tool discovery and invocation:

- **MCP Client Initialization:** For each plugin enabled in a given context (workspace/project), the platform’s service layer will initialize an **MCP client** instance (using, for instance, the official Python MCP SDK) to interface with that plugin’s server. This typically happens at startup or when a plugin is enabled. During initialization, the client and server perform a handshake exchanging protocol versions and basic info. In an MCP architecture, the Host (our platform) may spawn multiple client connections – one per plugin server – each maintaining a 1:1 link . We ensure each MCP client knows the workspace context (if needed for logging or auth) but importantly, it knows the plugin’s endpoint from the cached metadata.

- **Capability Discovery:** After handshake, the platform (as the client) requests the list of available capabilities from the plugin server (the **discovery phase**). The MCP server responds with a list of Tools (and any Resources/Prompts) it offers, including for each a name, description, and the input/output schema or interface . The platform uses this response to **register the tools in the LLM’s agent context**. In practice, if using LangChain, we will create a LangChain Tool object for each plugin action. For example, if the GitHub plugin lists a tool fetch_open_issues(repo_name), we create a Tool in the agent with that name and a function that, when called, will send an MCP request to the plugin. These tool definitions (name and description) can be given to the LLM (either as part of the prompt if using ReAct-style agents or via function-calling interfaces if using OpenAI function calling etc.) so the LLM is aware that such actions are available. This discovery step can be done on system startup and cached, so that for subsequent queries we don’t need to fetch the list every time. Essentially, the plugin’s toolset is loaded into memory as part of the project/workspace agent setup.

- **LLM Agent and Tool Use:** When a client query comes in to the FastAPI (say the user asks a question), the platform will determine which Project it’s for and thereby which plugins (tools) are enabled in that context. It will then invoke the appropriate LLM chain. Typically, this could be an agent chain (LangChain’s Agent) that has access to tools:

1. First, the agent will likely use the Project’s knowledge base: e.g., use LlamaIndex/PGVector to retrieve relevant documents from the vector store (this could itself be implemented as a Tool or as a preliminary step providing context). Often, we might use a **Retrieval-Augmented Generation (RAG)** approach: fetch top-k similar docs via PGVector and feed them into the prompt.

2. The agent then has the user query (and maybe retrieved context). If the LLM alone can answer, it will. But if the query requires an external action (e.g. “Schedule a meeting next week” might require calling a calendar plugin), the agent can decide to invoke a tool. LangChain’s agent will produce a decision to call a tool with certain arguments. The platform intercepts this and matches the tool name to one of the plugin Tools loaded earlier.

3. **Tool Invocation:** The MCP client sends a request to the plugin server for the chosen tool, with the given parameters. For instance, if the LLM decided to call fetch_open_issues("my-repo"), the MCP client will send a request to the GitHub plugin’s endpoint (likely via an HTTP POST or a websocket message following MCP’s protocol) to execute the fetch_open_issues action. This is done through the MCP client object, abstracting the low-level details of transport. The client correlates this request to a future response.

4. **Plugin Execution:** On the plugin side (MCP server), it receives the request, executes the logic (e.g., calls GitHub API to get open issues), and then sends back a response with the result data . The communication is asynchronous but appears synchronous to the agent: the MCP client waits (with a timeout) for the response.

5. **Result Handling:** The platform receives the tool result via the MCP client and passes it back into the LangChain agent as the observation/result of that tool action. The LLM then continues its reasoning with that new information. For example, it might incorporate the fetched data into its answer. This loop can repeat if the LLM decides to use multiple tools in sequence (the agent can call one tool, get info, then call another, etc., until it decides to finalize an answer).

6. Finally, the LLM produces an answer which the platform returns via the FastAPI response to the client application.

During tool invocation, the system takes care of formatting the inputs/outputs properly (this may involve serializing to JSON if using JSON-over-SSE as MCP might do, or just function call abstraction). We may also use **LangChain’s toolkit integration for Connery/MCP** to streamline this. (LangChain JS has a Connery toolkit, and conceptually we’d do similar in Python: wrapping an MCP tool as a LangChain Tool is straightforward since it’s just calling a Python function which internally does an HTTP request to the plugin).

- **Performance and Connection Management:** To avoid startup overhead on each request, the platform will ideally maintain persistent connections to plugin servers. For example, if plugins communicate via Server-Sent Events (SSE) or a persistent HTTP connection, we can keep that open in the MCP client for the lifetime of the application (or reconnect as needed). This means when a tool is invoked, it’s a matter of sending a message over an existing connection rather than establishing a new one. The cached endpoint and client instance mentioned earlier facilitate this. If a plugin is seldom used, we might lazily initialize it on first use (delaying the cost of launching it), but once started, reuse it for subsequent calls.

- **Error Handling and Security:** The design anticipates that plugin calls might fail or take time. The MCP client will enforce timeouts – if a plugin doesn’t respond in a reasonable time, the platform will catch that and handle it (possibly by telling the LLM the tool failed, so the agent can decide to try something else or apologize). Also, each plugin runs in isolation, so a crash or error in one plugin does not bring down the main platform – the MCP client would just receive an error and we can log it. On security: since plugins can perform powerful actions (like “send email” or database writes), enabling them is an admin action. The platform should ensure that only trusted plugins are installed, and perhaps provide an allow-list or sandboxing. In an MCP context, the Host can also attach an **authorization context** to the client (for example, some plugins might require an API key; the platform will supply it via environment variable or an auth handshake).

- **Global vs Project Plugin Invocation:** If a plugin is enabled globally for a workspace, the agent in any project can use it. The actual invocation path is the same, but it means those tools are loaded into all project agents. If a plugin is enabled only for a specific project, only that project’s agent knows about those tools. This is enforced simply by only loading the tools in the contexts where they are enabled.

In summary, the plugin mechanism extends the base system by allowing dynamic “function calling” to external services through a standardized MCP interface. This turns the platform into a **hub for AI plugins**, much like Connery’s vision of community-driven actions. The LLM can thus do far more than answer questions: it can take real actions (search the web, modify data, integrate with SaaS apps) as long as there’s a plugin for it and it’s authorized in the workspace. All of this is done in a controlled way, with the platform mediating every call (providing logging, potential moderation, and multi-tenant safety by not mixing data across workspaces).
